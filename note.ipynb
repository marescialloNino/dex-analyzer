{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solana and Ethereum Liquidity Pool Analysis\n",
    "\n",
    "This notebook fetches and analyzes liquidity pool data from decentralized exchanges (DEXes) on Solana and Ethereum blockchains using the GeckoTerminal API. It allows interactive selection of chains, DEXes, and filtering parameters such as TVL and volume ranges.\n",
    "\n",
    "## Setup\n",
    "Install dependencies and import required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.output {max-height: 500px; overflow-y: auto;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906f74175fcf4039911b32d6f2ebde3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(SelectMultiple(description='Chains:', index=(0,), options=('solana', 'eth', 'bsc', 'polygon_pos…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7c442323454dcebfb7515a4aa3d277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=True, description='Use All Pools for Correlations'), SelectMultiple(description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from src.clients.geckoterminal import GeckoTerminalClient\n",
    "from src.constants import Network, NETWORK_CONFIG\n",
    "from src.utils.analyzer import Analyzer\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "# Initialize styles for better output display\n",
    "display(HTML('<style>.output {max-height: 500px; overflow-y: auto;}</style>'))\n",
    "\n",
    "# Define available chains and DEXes\n",
    "chains = [Network.SOLANA.value, Network.ETHEREUM.value, Network.BSC.value, Network.POLYGONPOS.value,\n",
    "          Network.ARBITRUM.value, Network.SUI.value, Network.BASE.value]\n",
    "dexes_by_chain = {\n",
    "    chain: NETWORK_CONFIG[chain]['dexes'] for chain in chains\n",
    "}\n",
    "\n",
    "# Create widgets with multiple selection\n",
    "chain_dropdown = widgets.SelectMultiple(\n",
    "    options=chains, \n",
    "    description='Chains:', \n",
    "    value=[chains[0]],\n",
    "    rows=5\n",
    ")\n",
    "dex_dropdown = widgets.SelectMultiple(\n",
    "    options=dexes_by_chain[chains[0]], \n",
    "    description='DEXes:', \n",
    "    value=[],\n",
    "    rows=5\n",
    ")\n",
    "min_tvl = widgets.FloatText(value=10000, description='Min TVL ($):')\n",
    "max_tvl = widgets.FloatText(value=10000000000.0, description='Max TVL ($):')\n",
    "min_volume = widgets.FloatText(value=5000, description='Min Volume ($):')\n",
    "no_pivots = widgets.Checkbox(value=False, description='Exclude Pivot Tokens')\n",
    "no_stables = widgets.Checkbox(value=True, description='Exclude Stablecoins')\n",
    "utility_pairs = widgets.Checkbox(value=False, description='Utility Pairs Only')\n",
    "force_redownload = widgets.Checkbox(value=False, description='Force Re-Download Data')\n",
    "fetch_button = widgets.Button(description='Fetch Pools', button_style='success')\n",
    "output = widgets.Output()\n",
    "\n",
    "# Update DEX dropdown based on chain selection (for multiple chains, show union of DEXes)\n",
    "def update_dex_dropdown(change):\n",
    "    selected_chains = change['new']\n",
    "    all_dexes = set()\n",
    "    for chain in selected_chains:\n",
    "        all_dexes.update(dexes_by_chain.get(chain, []))\n",
    "    dex_dropdown.options = sorted(all_dexes)\n",
    "chain_dropdown.observe(update_dex_dropdown, names='value')\n",
    "\n",
    "# Display widgets\n",
    "display(widgets.VBox([\n",
    "    chain_dropdown, dex_dropdown, min_tvl, max_tvl, min_volume,\n",
    "    no_pivots, no_stables, utility_pairs, force_redownload, fetch_button, output\n",
    "]))\n",
    "\n",
    "# Define fetch_pools function\n",
    "def fetch_pools(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print('Starting fetch_pools function...')\n",
    "        try:\n",
    "            selected_chains = list(chain_dropdown.value)\n",
    "            selected_dexes = set(dex_dropdown.value)  # Use set for faster lookup\n",
    "            \n",
    "            if not selected_chains:\n",
    "                print(\"No chains selected.\")\n",
    "                return\n",
    "            \n",
    "            all_pools = []\n",
    "            for chain in selected_chains:\n",
    "                print(f\"Processing chain: {chain}\")\n",
    "                chain_dexes = dexes_by_chain.get(chain, [])\n",
    "                \n",
    "                # If no specific DEXes selected, fetch from all DEXes for the chain\n",
    "                dexes_to_fetch = [dex for dex in chain_dexes if dex in selected_dexes] if selected_dexes else chain_dexes\n",
    "                \n",
    "                if not dexes_to_fetch:\n",
    "                    print(f\"No DEXes to fetch for chain {chain}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                for dex_name in dexes_to_fetch:\n",
    "                    print(f\"Fetching from DEX: {dex_name} on chain {chain}\")\n",
    "                    \n",
    "                    # Initialize client with DEX name as string\n",
    "                    client = GeckoTerminalClient(\n",
    "                        network=chain,\n",
    "                        dex=dex_name\n",
    "                    )\n",
    "                    \n",
    "                    # Fetch pools\n",
    "                    pools = client.fetch_liquidity_pools(\n",
    "                        all_pages=True,\n",
    "                        min_tvl=min_tvl.value,\n",
    "                        max_tvl=max_tvl.value,\n",
    "                        min_volume=min_volume.value,\n",
    "                        no_pivots=no_pivots.value,\n",
    "                        no_stables=no_stables.value,\n",
    "                        utility_pairs=utility_pairs.value\n",
    "                    )\n",
    "                    \n",
    "                    # Filter pools that are at least 2 weeks old and save data\n",
    "                    filtered_pools = []\n",
    "                    for pool in pools:\n",
    "                        pool.chain = chain\n",
    "                        pool.dex = dex_name\n",
    "                        \n",
    "                        # Create data folder structure\n",
    "                        pair = f'{pool.token0_symbol}_{pool.token1_symbol}'\n",
    "                        data_folder = os.path.join('data', chain, dex_name, pair)\n",
    "                        os.makedirs(data_folder, exist_ok=True)\n",
    "                        \n",
    "                        base_usd_path = os.path.join(data_folder, 'base_usd.csv')\n",
    "                        quote_usd_path = os.path.join(data_folder, 'quote_usd.csv')\n",
    "                        cross_path = os.path.join(data_folder, 'cross.csv')\n",
    "                        \n",
    "                        # Check if data exists and not forcing re-download\n",
    "                        if not force_redownload.value and os.path.exists(base_usd_path) and os.path.exists(quote_usd_path) and os.path.exists(cross_path):\n",
    "                            print(f\"Loading existing data for {pair} from {data_folder}\")\n",
    "                            base_df = pd.read_csv(base_usd_path)\n",
    "                            quote_df = pd.read_csv(quote_usd_path)\n",
    "                            cross_df = pd.read_csv(cross_path)\n",
    "                        else:\n",
    "                            print(f\"Fetching data for {pair}\")\n",
    "                            # Fetch USD bars for base token\n",
    "                            base_usd_bars = client.get_price_bars(\n",
    "                                pool_address=pool.address,\n",
    "                                timeframe='hour',\n",
    "                                aggregate=1,\n",
    "                                currency='usd',\n",
    "                                token='base',\n",
    "                                limit=1000\n",
    "                            )\n",
    "                            \n",
    "                            # Fetch USD bars for quote token\n",
    "                            quote_usd_bars = client.get_price_bars(\n",
    "                                pool_address=pool.address,\n",
    "                                timeframe='hour',\n",
    "                                aggregate=1,\n",
    "                                currency='usd',\n",
    "                                token='quote',\n",
    "                                limit=1000\n",
    "                            )\n",
    "                            \n",
    "                            # Fetch cross price bars (base in quote token)\n",
    "                            cross_bars = client.get_price_bars(\n",
    "                                pool_address=pool.address,\n",
    "                                timeframe='hour',\n",
    "                                aggregate=1,\n",
    "                                currency='token',\n",
    "                                token='base',\n",
    "                                limit=1000\n",
    "                            )\n",
    "                            \n",
    "                            if base_usd_bars and quote_usd_bars and cross_bars:\n",
    "                                base_df = base_usd_bars.data\n",
    "                                quote_df = quote_usd_bars.data\n",
    "                                cross_df = cross_bars.data\n",
    "                                \n",
    "                                # Save to CSV\n",
    "                                base_df.to_csv(base_usd_path, index=False)\n",
    "                                quote_df.to_csv(quote_usd_path, index=False)\n",
    "                                cross_df.to_csv(cross_path, index=False)\n",
    "                                print(f\"Saved data for {pair} to {data_folder}\")\n",
    "                            else:\n",
    "                                print(f\"Failed to fetch data for {pair}. Skipping.\")\n",
    "                                continue\n",
    "                        \n",
    "                        # Check pool age (at least 2 weeks old)\n",
    "                        # Assuming timeframe='hour', aggregate=1 (1 hour intervals), check if data spans at least 336 hours (14 days * 24 hours)\n",
    "                        if len(cross_df) < 336:  # Minimum bars for 2 weeks at 1h interval\n",
    "                            print(f\"Pool {pair} is too young (less than 2 weeks of data). Discarding.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Calculate time span\n",
    "                        time_span_hours = (cross_df['timestamp'].max() - cross_df['timestamp'].min()).total_seconds() / 3600\n",
    "                        if time_span_hours < 336:\n",
    "                            print(f\"Pool {pair} data spans only {time_span_hours:.2f} hours (< 2 weeks). Discarding.\")\n",
    "                            continue\n",
    "                        \n",
    "                        filtered_pools.append(pool)\n",
    "                        print(f\"Added pool {pair} (age OK)\")\n",
    "                    \n",
    "                    all_pools.extend(filtered_pools)\n",
    "                    print(f\"Filtered {len(filtered_pools)} pools (age >= 2 weeks) from {dex_name} on {chain}\")\n",
    "            \n",
    "            if not all_pools:\n",
    "                print('No pools found matching the criteria.')\n",
    "                return\n",
    "            \n",
    "            print(f\"Total filtered pools: {len(all_pools)}\")\n",
    "            # Convert to DataFrame with added chain and DEX columns\n",
    "            df = pd.DataFrame([\n",
    "                {\n",
    "                    'Chain': pool.chain,\n",
    "                    'DEX': pool.dex,\n",
    "                    'Pair': f'{pool.token0_symbol}/{pool.token1_symbol}',\n",
    "                    'TVL': pool.tvl,\n",
    "                    'Volume': pool.volume,\n",
    "                    'Address': pool.address\n",
    "                } for pool in all_pools\n",
    "            ])\n",
    "            \n",
    "            # Display results\n",
    "            print(f'Displaying {len(all_pools)} pools:')\n",
    "            display(df)\n",
    "            \n",
    "            # Save to CSV\n",
    "            output_file = f'pools_multi_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "            df.to_csv(output_file, index=False)\n",
    "            print(f'Saved results to {output_file}')\n",
    "            \n",
    "            # Visualize TVL and Volume\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.hist(df['TVL'], bins=20, color='blue', alpha=0.7)\n",
    "            plt.title('TVL Distribution')\n",
    "            plt.xlabel('TVL ($)')\n",
    "            plt.ylabel('Count')\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.hist(df['Volume'], bins=20, color='green', alpha=0.7)\n",
    "            plt.title('Volume Distribution')\n",
    "            plt.xlabel('Volume ($)')\n",
    "            plt.ylabel('Count')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Error fetching pools: {e}')\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Register button click handler\n",
    "fetch_button.on_click(fetch_pools)\n",
    "\n",
    "# Widgets for price correlation analysis\n",
    "pool_select = widgets.SelectMultiple(\n",
    "    options=[],\n",
    "    description='Select Pools:',\n",
    "    rows=10\n",
    ")\n",
    "days = widgets.IntSlider(value=7, min=1, max=30, description='Days:')\n",
    "use_all_pools_checkbox = widgets.Checkbox(value=True, description='Use All Pools for Correlations')\n",
    "analyze_button = widgets.Button(description='Analyze Correlations', button_style='info')\n",
    "corr_output = widgets.Output()\n",
    "stats_output = widgets.Output()\n",
    "\n",
    "# Display correlation widgets\n",
    "display(widgets.VBox([use_all_pools_checkbox, pool_select, days, analyze_button, corr_output, stats_output]))\n",
    "\n",
    "# Update pool options and calculate stats after fetching\n",
    "def update_pool_options_and_calculate_stats(change):\n",
    "    with output:\n",
    "        print(\"Updating pool options and calculating stats...\")\n",
    "        try:\n",
    "            # Look for the latest CSV\n",
    "            csv_files = glob.glob('pools_multi_*.csv')\n",
    "            if not csv_files:\n",
    "                print(\"No pools CSV found. Please fetch pools first.\")\n",
    "                return\n",
    "            latest_csv = max(csv_files, key=os.path.getctime)\n",
    "            df = pd.read_csv(latest_csv)\n",
    "            pool_select.options = [f'{row[\"Chain\"]}/{row[\"DEX\"]}/{row[\"Pair\"]} ({row[\"Address\"][:8]}...)' for _, row in df.iterrows()]\n",
    "            print(f\"Loaded {len(pool_select.options)} pools from {latest_csv} for selection.\")\n",
    "            \n",
    "            # Calculate stats for all pools using saved data\n",
    "            with stats_output:\n",
    "                stats_output.clear_output()\n",
    "                print(\"Calculating metrics for all pools...\")\n",
    "                metrics_list = []\n",
    "                for _, row in df.iterrows():\n",
    "                    chain = row['Chain']\n",
    "                    dex_name = row['DEX']\n",
    "                    address = row['Address']\n",
    "                    tvl = row['TVL']\n",
    "                    pair = row['Pair']\n",
    "                    token0_symbol, token1_symbol = pair.split('/')\n",
    "                    \n",
    "                    # Load from data folder\n",
    "                    data_folder = os.path.join('data', chain, dex_name, f'{token0_symbol}_{token1_symbol}')\n",
    "                    base_usd_path = os.path.join(data_folder, 'base_usd.csv')\n",
    "                    quote_usd_path = os.path.join(data_folder, 'quote_usd.csv')\n",
    "                    cross_path = os.path.join(data_folder, 'cross.csv')\n",
    "                    \n",
    "                    if os.path.exists(base_usd_path) and os.path.exists(quote_usd_path) and os.path.exists(cross_path):\n",
    "                        base_df = pd.read_csv(base_usd_path)\n",
    "                        quote_df = pd.read_csv(quote_usd_path)\n",
    "                        cross_df = pd.read_csv(cross_path)\n",
    "                        \n",
    "                        metrics = calculate_pool_metrics(\n",
    "                            tvl=tvl,\n",
    "                            base_df=base_df,\n",
    "                            cross_price_df=cross_df,\n",
    "                            days_interval=days.value\n",
    "                        )\n",
    "                        metrics['Chain'] = chain\n",
    "                        metrics['DEX'] = dex_name\n",
    "                        metrics['Pair'] = pair\n",
    "                        metrics['Address'] = address\n",
    "                        metrics['TVL'] = tvl\n",
    "                        metrics_list.append(metrics)\n",
    "                    else:\n",
    "                        print(f\"Data not found for {pair} on {chain}/{dex_name}. Skipping metrics.\")\n",
    "                \n",
    "                if metrics_list:\n",
    "                    stats_df = pd.DataFrame(metrics_list)\n",
    "                    print(\"Pool Metrics:\")\n",
    "                    display(stats_df)\n",
    "                else:\n",
    "                    print(\"No metrics calculated.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating pool options or calculating stats: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "fetch_button.on_click(update_pool_options_and_calculate_stats)\n",
    "\n",
    "# Analyze correlations\n",
    "def analyze_correlations(b):\n",
    "    with corr_output:\n",
    "        corr_output.clear_output()\n",
    "        print(\"Starting correlation analysis...\")\n",
    "        selected_pools = pool_select.value if not use_all_pools_checkbox.value else pool_select.options\n",
    "        \n",
    "        if not selected_pools:\n",
    "            print('Please select at least one pool or check \"Use All Pools\".')\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            data_list = []\n",
    "            for pool_str in selected_pools:\n",
    "                print(f\"Processing pool: {pool_str}\")\n",
    "                # Parse chain, DEX, pair, address from the option string\n",
    "                parts = pool_str.split('/')\n",
    "                if len(parts) < 3:\n",
    "                    print(f\"Invalid pool format: {pool_str}. Skipping.\")\n",
    "                    continue\n",
    "                chain = parts[0]\n",
    "                dex_name = parts[1]\n",
    "                pair_address_str = '/'.join(parts[2:])\n",
    "                pair = pair_address_str.split('(')[0].strip()\n",
    "                address = pair_address_str.split('(')[1][:-1].split('...')[0]\n",
    "                \n",
    "                token_symbol = pair.split('/')[0]  # Assume first token for simplicity\n",
    "                \n",
    "                # Load from data folder for cross prices (for correlation, we use close prices)\n",
    "                token0_symbol, token1_symbol = pair.split('/')\n",
    "                data_folder = os.path.join('data', chain, dex_name, f'{token0_symbol}_{token1_symbol}')\n",
    "                cross_path = os.path.join(data_folder, 'cross.csv')\n",
    "                \n",
    "                if os.path.exists(cross_path):\n",
    "                    cross_df = pd.read_csv(cross_path)\n",
    "                    data_list.append((token_symbol, cross_df))\n",
    "                    print(f\"Loaded {len(cross_df)} bars for {token_symbol} from saved data.\")\n",
    "                else:\n",
    "                    print(f\"Cross data not found for {pair}. Skipping.\")\n",
    "            \n",
    "            if not data_list:\n",
    "                print('No price data available for selected pools.')\n",
    "                return\n",
    "            \n",
    "            corr_matrix = Analyzer.compute_correlation_matrix_from_dataframes(data_list)\n",
    "            if not corr_matrix.empty:\n",
    "                print('Correlation Matrix:')\n",
    "                display(corr_matrix)\n",
    "                \n",
    "                # Visualize correlation matrix\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')\n",
    "                plt.colorbar(label='Correlation')\n",
    "                plt.xticks(range(len(corr_matrix)), corr_matrix.columns, rotation=45)\n",
    "                plt.yticks(range(len(corr_matrix)), corr_matrix.index)\n",
    "                plt.title('Price Correlation Matrix')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print('No correlation data available.')\n",
    "        except Exception as e:\n",
    "            print(f'Error analyzing correlations: {e}')\n",
    "            traceback.print_exc()\n",
    "\n",
    "analyze_button.on_click(analyze_correlations)\n",
    "\n",
    "# Automatically try to update pool options and calculate stats if CSV exists\n",
    "update_pool_options_and_calculate_stats(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Select the blockchain, DEX, and filtering parameters using interactive widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.output {max-height: 500px; overflow-y: auto;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fd1565c3ac427f9cf5d2752e86e4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(SelectMultiple(description='Chains:', index=(0,), options=('solana', 'eth', 'bsc', 'polygon_pos…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d6aa2c6750482e913a2df8f2991b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=True, description='Use All Pools for Correlations'), SelectMultiple(description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from src.clients.geckoterminal import GeckoTerminalClient\n",
    "from src.constants import Network, NETWORK_CONFIG\n",
    "from src.utils.analyzer import Analyzer\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "# Initialize styles for better output display\n",
    "display(HTML('<style>.output {max-height: 500px; overflow-y: auto;}</style>'))\n",
    "\n",
    "# Define available chains and DEXes\n",
    "chains = [Network.SOLANA.value, Network.ETHEREUM.value, Network.BSC.value, Network.POLYGONPOS.value,\n",
    "          Network.ARBITRUM.value, Network.SUI.value, Network.BASE.value]\n",
    "dexes_by_chain = {\n",
    "    chain: NETWORK_CONFIG[chain]['dexes'] for chain in chains\n",
    "}\n",
    "\n",
    "# Create widgets with multiple selection\n",
    "chain_dropdown = widgets.SelectMultiple(\n",
    "    options=chains, \n",
    "    description='Chains:', \n",
    "    value=[chains[0]],\n",
    "    rows=5\n",
    ")\n",
    "dex_dropdown = widgets.SelectMultiple(\n",
    "    options=dexes_by_chain[chains[0]], \n",
    "    description='DEXes:', \n",
    "    value=[],\n",
    "    rows=5\n",
    ")\n",
    "min_tvl = widgets.FloatText(value=10000, description='Min TVL ($):')\n",
    "max_tvl = widgets.FloatText(value=10000000000.0, description='Max TVL ($):')\n",
    "min_volume = widgets.FloatText(value=5000, description='Min Volume ($):')\n",
    "no_pivots = widgets.Checkbox(value=False, description='Exclude Pivot Tokens')\n",
    "no_stables = widgets.Checkbox(value=True, description='Exclude Stablecoins')\n",
    "utility_pairs = widgets.Checkbox(value=False, description='Utility Pairs Only')\n",
    "force_redownload = widgets.Checkbox(value=False, description='Force Re-Download Data')\n",
    "fetch_button = widgets.Button(description='Fetch Pools', button_style='success')\n",
    "output = widgets.Output()\n",
    "\n",
    "# Update DEX dropdown based on chain selection (for multiple chains, show union of DEXes)\n",
    "def update_dex_dropdown(change):\n",
    "    selected_chains = change['new']\n",
    "    all_dexes = set()\n",
    "    for chain in selected_chains:\n",
    "        all_dexes.update(dexes_by_chain.get(chain, []))\n",
    "    dex_dropdown.options = sorted(all_dexes)\n",
    "chain_dropdown.observe(update_dex_dropdown, names='value')\n",
    "\n",
    "# Display widgets\n",
    "display(widgets.VBox([\n",
    "    chain_dropdown, dex_dropdown, min_tvl, max_tvl, min_volume,\n",
    "    no_pivots, no_stables, utility_pairs, force_redownload, fetch_button, output\n",
    "]))\n",
    "\n",
    "# Define fetch_pools function\n",
    "def fetch_pools(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print('Starting fetch_pools function...')\n",
    "        try:\n",
    "            selected_chains = list(chain_dropdown.value)\n",
    "            selected_dexes = set(dex_dropdown.value)  # Use set for faster lookup\n",
    "            \n",
    "            if not selected_chains:\n",
    "                print(\"No chains selected.\")\n",
    "                return\n",
    "            \n",
    "            all_pools = []\n",
    "            for chain in selected_chains:\n",
    "                print(f\"Processing chain: {chain}\")\n",
    "                chain_dexes = dexes_by_chain.get(chain, [])\n",
    "                \n",
    "                # If no specific DEXes selected, fetch from all DEXes for the chain\n",
    "                dexes_to_fetch = [dex for dex in chain_dexes if dex in selected_dexes] if selected_dexes else chain_dexes\n",
    "                \n",
    "                if not dexes_to_fetch:\n",
    "                    print(f\"No DEXes to fetch for chain {chain}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                for dex_name in dexes_to_fetch:\n",
    "                    print(f\"Fetching from DEX: {dex_name} on chain {chain}\")\n",
    "                    \n",
    "                    # Initialize client with DEX name as string\n",
    "                    client = GeckoTerminalClient(\n",
    "                        network=chain,\n",
    "                        dex=dex_name\n",
    "                    )\n",
    "                    \n",
    "                    # Fetch pools\n",
    "                    pools = client.fetch_liquidity_pools(\n",
    "                        all_pages=True,\n",
    "                        min_tvl=min_tvl.value,\n",
    "                        max_tvl=max_tvl.value,\n",
    "                        min_volume=min_volume.value,\n",
    "                        no_pivots=no_pivots.value,\n",
    "                        no_stables=no_stables.value,\n",
    "                        utility_pairs=utility_pairs.value\n",
    "                    )\n",
    "                    \n",
    "                    # Filter pools that are at least 2 weeks old and save data\n",
    "                    filtered_pools = []\n",
    "                    for pool in pools:\n",
    "                        pool.chain = chain\n",
    "                        pool.dex = dex_name\n",
    "                        \n",
    "                        # Create data folder structure\n",
    "                        pair = f'{pool.token0_symbol}_{pool.token1_symbol}'\n",
    "                        data_folder = os.path.join('data', chain, dex_name, pair)\n",
    "                        os.makedirs(data_folder, exist_ok=True)\n",
    "                        \n",
    "                        base_usd_path = os.path.join(data_folder, 'base_usd.csv')\n",
    "                        quote_usd_path = os.path.join(data_folder, 'quote_usd.csv')\n",
    "                        cross_path = os.path.join(data_folder, 'cross.csv')\n",
    "                        \n",
    "                        # Check if data exists and not forcing re-download\n",
    "                        if not force_redownload.value and os.path.exists(base_usd_path) and os.path.exists(quote_usd_path) and os.path.exists(cross_path):\n",
    "                            print(f\"Loading existing data for {pair} from {data_folder}\")\n",
    "                            base_df = pd.read_csv(base_usd_path)\n",
    "                            quote_df = pd.read_csv(quote_usd_path)\n",
    "                            cross_df = pd.read_csv(cross_path)\n",
    "                        else:\n",
    "                            print(f\"Fetching data for {pair}\")\n",
    "                            # Fetch USD bars for base token\n",
    "                            base_usd_bars = client.get_price_bars(\n",
    "                                pool_address=pool.address,\n",
    "                                timeframe='hour',\n",
    "                                aggregate=1,\n",
    "                                currency='usd',\n",
    "                                token='base',\n",
    "                                limit=1000\n",
    "                            )\n",
    "                            \n",
    "                            # Fetch USD bars for quote token\n",
    "                            quote_usd_bars = client.get_price_bars(\n",
    "                                pool_address=pool.address,\n",
    "                                timeframe='hour',\n",
    "                                aggregate=1,\n",
    "                                currency='usd',\n",
    "                                token='quote',\n",
    "                                limit=1000\n",
    "                            )\n",
    "                            \n",
    "                            # Fetch cross price bars (base in quote token)\n",
    "                            cross_bars = client.get_price_bars(\n",
    "                                pool_address=pool.address,\n",
    "                                timeframe='hour',\n",
    "                                aggregate=1,\n",
    "                                currency='token',\n",
    "                                token='base',\n",
    "                                limit=1000\n",
    "                            )\n",
    "                            \n",
    "                            if base_usd_bars and quote_usd_bars and cross_bars:\n",
    "                                base_df = base_usd_bars.data\n",
    "                                quote_df = quote_usd_bars.data\n",
    "                                cross_df = cross_bars.data\n",
    "                                \n",
    "                                # Save to CSV\n",
    "                                base_df.to_csv(base_usd_path, index=False)\n",
    "                                quote_df.to_csv(quote_usd_path, index=False)\n",
    "                                cross_df.to_csv(cross_path, index=False)\n",
    "                                print(f\"Saved data for {pair} to {data_folder}\")\n",
    "                            else:\n",
    "                                print(f\"Failed to fetch data for {pair}. Skipping.\")\n",
    "                                continue\n",
    "                        \n",
    "                        # Check pool age (at least 2 weeks old)\n",
    "                        # Assuming timeframe='hour', aggregate=1 (1 hour intervals), check if data spans at least 336 hours (14 days * 24 hours)\n",
    "                        if len(cross_df) < 336:  # Minimum bars for 2 weeks at 1h interval\n",
    "                            print(f\"Pool {pair} is too young (less than 2 weeks of data). Discarding.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Calculate time span\n",
    "                        time_span_hours = (cross_df['timestamp'].max() - cross_df['timestamp'].min()).total_seconds() / 3600\n",
    "                        if time_span_hours < 336:\n",
    "                            print(f\"Pool {pair} data spans only {time_span_hours:.2f} hours (< 2 weeks). Discarding.\")\n",
    "                            continue\n",
    "                        \n",
    "                        filtered_pools.append(pool)\n",
    "                        print(f\"Added pool {pair} (age OK)\")\n",
    "                    \n",
    "                    all_pools.extend(filtered_pools)\n",
    "                    print(f\"Filtered {len(filtered_pools)} pools (age >= 2 weeks) from {dex_name} on {chain}\")\n",
    "            \n",
    "            if not all_pools:\n",
    "                print('No pools found matching the criteria.')\n",
    "                return\n",
    "            \n",
    "            print(f\"Total filtered pools: {len(all_pools)}\")\n",
    "            # Convert to DataFrame with added chain and DEX columns\n",
    "            df = pd.DataFrame([\n",
    "                {\n",
    "                    'Chain': pool.chain,\n",
    "                    'DEX': pool.dex,\n",
    "                    'Pair': f'{pool.token0_symbol}/{pool.token1_symbol}',\n",
    "                    'TVL': pool.tvl,\n",
    "                    'Volume': pool.volume,\n",
    "                    'Address': pool.address\n",
    "                } for pool in all_pools\n",
    "            ])\n",
    "            \n",
    "            # Display results\n",
    "            print(f'Displaying {len(all_pools)} pools:')\n",
    "            display(df)\n",
    "            \n",
    "            # Save to CSV\n",
    "            output_file = f'pools_multi_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "            df.to_csv(output_file, index=False)\n",
    "            print(f'Saved results to {output_file}')\n",
    "            \n",
    "            # Visualize TVL and Volume\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.hist(df['TVL'], bins=20, color='blue', alpha=0.7)\n",
    "            plt.title('TVL Distribution')\n",
    "            plt.xlabel('TVL ($)')\n",
    "            plt.ylabel('Count')\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.hist(df['Volume'], bins=20, color='green', alpha=0.7)\n",
    "            plt.title('Volume Distribution')\n",
    "            plt.xlabel('Volume ($)')\n",
    "            plt.ylabel('Count')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Error fetching pools: {e}')\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Register button click handler\n",
    "fetch_button.on_click(fetch_pools)\n",
    "\n",
    "# Widgets for price correlation analysis\n",
    "pool_select = widgets.SelectMultiple(\n",
    "    options=[],\n",
    "    description='Select Pools:',\n",
    "    rows=10\n",
    ")\n",
    "days = widgets.IntSlider(value=7, min=1, max=30, description='Days:')\n",
    "use_all_pools_checkbox = widgets.Checkbox(value=True, description='Use All Pools for Correlations')\n",
    "analyze_button = widgets.Button(description='Analyze Correlations', button_style='info')\n",
    "corr_output = widgets.Output()\n",
    "stats_output = widgets.Output()\n",
    "\n",
    "# Display correlation widgets\n",
    "display(widgets.VBox([use_all_pools_checkbox, pool_select, days, analyze_button, corr_output, stats_output]))\n",
    "\n",
    "# Update pool options and calculate stats after fetching\n",
    "def update_pool_options_and_calculate_stats(change):\n",
    "    with output:\n",
    "        print(\"Updating pool options and calculating stats...\")\n",
    "        try:\n",
    "            # Look for the latest CSV\n",
    "            csv_files = glob.glob('pools_multi_*.csv')\n",
    "            if not csv_files:\n",
    "                print(\"No pools CSV found. Please fetch pools first.\")\n",
    "                return\n",
    "            latest_csv = max(csv_files, key=os.path.getctime)\n",
    "            df = pd.read_csv(latest_csv)\n",
    "            pool_select.options = [f'{row[\"Chain\"]}/{row[\"DEX\"]}/{row[\"Pair\"]} ({row[\"Address\"][:8]}...)' for _, row in df.iterrows()]\n",
    "            print(f\"Loaded {len(pool_select.options)} pools from {latest_csv} for selection.\")\n",
    "            \n",
    "            # Calculate stats for all pools using saved data\n",
    "            with stats_output:\n",
    "                stats_output.clear_output()\n",
    "                print(\"Calculating metrics for all pools...\")\n",
    "                metrics_list = []\n",
    "                for _, row in df.iterrows():\n",
    "                    chain = row['Chain']\n",
    "                    dex_name = row['DEX']\n",
    "                    address = row['Address']\n",
    "                    tvl = row['TVL']\n",
    "                    pair = row['Pair']\n",
    "                    token0_symbol, token1_symbol = pair.split('/')\n",
    "                    \n",
    "                    # Load from data folder\n",
    "                    data_folder = os.path.join('data', chain, dex_name, f'{token0_symbol}_{token1_symbol}')\n",
    "                    base_usd_path = os.path.join(data_folder, 'base_usd.csv')\n",
    "                    quote_usd_path = os.path.join(data_folder, 'quote_usd.csv')\n",
    "                    cross_path = os.path.join(data_folder, 'cross.csv')\n",
    "                    \n",
    "                    if os.path.exists(base_usd_path) and os.path.exists(quote_usd_path) and os.path.exists(cross_path):\n",
    "                        base_df = pd.read_csv(base_usd_path)\n",
    "                        quote_df = pd.read_csv(quote_usd_path)\n",
    "                        cross_df = pd.read_csv(cross_path)\n",
    "                        \n",
    "                        metrics = calculate_pool_metrics(\n",
    "                            tvl=tvl,\n",
    "                            base_df=base_df,\n",
    "                            cross_price_df=cross_df,\n",
    "                            days_interval=days.value\n",
    "                        )\n",
    "                        metrics['Chain'] = chain\n",
    "                        metrics['DEX'] = dex_name\n",
    "                        metrics['Pair'] = pair\n",
    "                        metrics['Address'] = address\n",
    "                        metrics['TVL'] = tvl\n",
    "                        metrics_list.append(metrics)\n",
    "                    else:\n",
    "                        print(f\"Data not found for {pair} on {chain}/{dex_name}. Skipping metrics.\")\n",
    "                \n",
    "                if metrics_list:\n",
    "                    stats_df = pd.DataFrame(metrics_list)\n",
    "                    print(\"Pool Metrics:\")\n",
    "                    display(stats_df)\n",
    "                else:\n",
    "                    print(\"No metrics calculated.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating pool options or calculating stats: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "fetch_button.on_click(update_pool_options_and_calculate_stats)\n",
    "\n",
    "# Analyze correlations\n",
    "def analyze_correlations(b):\n",
    "    with corr_output:\n",
    "        corr_output.clear_output()\n",
    "        print(\"Starting correlation analysis...\")\n",
    "        selected_pools = pool_select.value if not use_all_pools_checkbox.value else pool_select.options\n",
    "        \n",
    "        if not selected_pools:\n",
    "            print('Please select at least one pool or check \"Use All Pools\".')\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            data_list = []\n",
    "            for pool_str in selected_pools:\n",
    "                print(f\"Processing pool: {pool_str}\")\n",
    "                # Parse chain, DEX, pair, address from the option string\n",
    "                parts = pool_str.split('/')\n",
    "                if len(parts) < 3:\n",
    "                    print(f\"Invalid pool format: {pool_str}. Skipping.\")\n",
    "                    continue\n",
    "                chain = parts[0]\n",
    "                dex_name = parts[1]\n",
    "                pair_address_str = '/'.join(parts[2:])\n",
    "                pair = pair_address_str.split('(')[0].strip()\n",
    "                address = pair_address_str.split('(')[1][:-1].split('...')[0]\n",
    "                \n",
    "                token_symbol = pair.split('/')[0]  # Assume first token for simplicity\n",
    "                \n",
    "                # Load from data folder for cross prices (for correlation, we use close prices)\n",
    "                token0_symbol, token1_symbol = pair.split('/')\n",
    "                data_folder = os.path.join('data', chain, dex_name, f'{token0_symbol}_{token1_symbol}')\n",
    "                cross_path = os.path.join(data_folder, 'cross.csv')\n",
    "                \n",
    "                if os.path.exists(cross_path):\n",
    "                    cross_df = pd.read_csv(cross_path)\n",
    "                    data_list.append((token_symbol, cross_df))\n",
    "                    print(f\"Loaded {len(cross_df)} bars for {token_symbol} from saved data.\")\n",
    "                else:\n",
    "                    print(f\"Cross data not found for {pair}. Skipping.\")\n",
    "            \n",
    "            if not data_list:\n",
    "                print('No price data available for selected pools.')\n",
    "                return\n",
    "            \n",
    "            corr_matrix = Analyzer.compute_correlation_matrix_from_dataframes(data_list)\n",
    "            if not corr_matrix.empty:\n",
    "                print('Correlation Matrix:')\n",
    "                display(corr_matrix)\n",
    "                \n",
    "                # Visualize correlation matrix\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')\n",
    "                plt.colorbar(label='Correlation')\n",
    "                plt.xticks(range(len(corr_matrix)), corr_matrix.columns, rotation=45)\n",
    "                plt.yticks(range(len(corr_matrix)), corr_matrix.index)\n",
    "                plt.title('Price Correlation Matrix')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print('No correlation data available.')\n",
    "        except Exception as e:\n",
    "            print(f'Error analyzing correlations: {e}')\n",
    "            traceback.print_exc()\n",
    "\n",
    "analyze_button.on_click(analyze_correlations)\n",
    "\n",
    "# Automatically try to update pool options and calculate stats if CSV exists\n",
    "update_pool_options_and_calculate_stats(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Liquidity Pools\n",
    "Fetch liquidity pools based on the selected parameters and display the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Price Correlations\n",
    "Select pools to fetch price data and compute a correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36e5b7d3cfd4c73ac67b089e71b6daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(SelectMultiple(description='Select Pools:', options=(), rows=10, value=()), IntSlider(value=7, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Widgets for price correlation analysis\n",
    "pool_select = widgets.SelectMultiple(\n",
    "    options=[],\n",
    "    description='Select Pools:',\n",
    "    rows=10\n",
    ")\n",
    "days = widgets.IntSlider(value=7, min=1, max=30, description='Days:')\n",
    "analyze_button = widgets.Button(description='Analyze Correlations', button_style='info')\n",
    "corr_output = widgets.Output()\n",
    "\n",
    "# Update pool options after fetching\n",
    "def update_pool_options(change):\n",
    "    with output:\n",
    "        print(\"Updating pool options...\")\n",
    "        try:\n",
    "            df = pd.read_csv(f'pools_{chain_dropdown.value}_{dex_dropdown.value}.csv')\n",
    "            pool_select.options = [f'{row[\"Pair\"]} ({row[\"Address\"]})' for _, row in df.iterrows()]\n",
    "            print(f\"Loaded {len(pool_select.options)} pools for selection.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pools CSV: {e}. Please fetch pools first.\")\n",
    "            pool_select.options = []\n",
    "\n",
    "fetch_button.on_click(update_pool_options)\n",
    "\n",
    "# Analyze correlations\n",
    "def analyze_correlations(b):\n",
    "    with corr_output:\n",
    "        corr_output.clear_output()\n",
    "        print(\"Starting correlation analysis...\")\n",
    "        if not pool_select.value:\n",
    "            print('Please select at least one pool.')\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            print(f\"Selected chain: {chain_dropdown.value}, DEX: {dex_dropdown.value}\")\n",
    "            \n",
    "            # Use DEX name as string directly\n",
    "            dex_name = dex_dropdown.value\n",
    "            \n",
    "            client = GeckoTerminalClient(\n",
    "                network=chain_dropdown.value,\n",
    "                dex=dex_name\n",
    "            )\n",
    "            print(\"GeckoTerminalClient initialized.\")\n",
    "            \n",
    "            data_list = []\n",
    "            for pool_str in pool_select.value:\n",
    "                print(f\"Processing pool: {pool_str}\")\n",
    "                address = pool_str.split('(')[1][:-1].split(')')[0]\n",
    "                pair = pool_str.split('(')[0].strip()\n",
    "                token_symbol = pair.split('/')[0]  # Assume first token for simplicity\n",
    "                from_date = (datetime.now() - timedelta(days=days.value)).strftime('%Y-%m-%d')\n",
    "                to_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                print(f\"Fetching price bars for {token_symbol} from {from_date} to {to_date}\")\n",
    "                price_bar = client.get_price_bars(\n",
    "                    pool_address=address,\n",
    "                    timeframe='hour',\n",
    "                    aggregate=4,  # For 4h intervals\n",
    "                    limit=1000\n",
    "                )\n",
    "                if price_bar and not price_bar.data.empty:\n",
    "                    data_list.append((token_symbol, price_bar.data))\n",
    "                    print(f\"Fetched {len(price_bar.data)} bars for {token_symbol}\")\n",
    "                else:\n",
    "                    print(f\"No price bars fetched for {token_symbol}\")\n",
    "            \n",
    "            if not data_list:\n",
    "                print('No price data available for selected pools.')\n",
    "                return\n",
    "            \n",
    "            corr_matrix = Analyzer.compute_correlation_matrix_from_dataframes(data_list)\n",
    "            if not corr_matrix.empty:\n",
    "                print('Correlation Matrix:')\n",
    "                display(corr_matrix)\n",
    "                \n",
    "                # Visualize correlation matrix\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')\n",
    "                plt.colorbar(label='Correlation')\n",
    "                plt.xticks(range(len(corr_matrix)), corr_matrix.columns, rotation=45)\n",
    "                plt.yticks(range(len(corr_matrix)), corr_matrix.index)\n",
    "                plt.title('Price Correlation Matrix')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print('No correlation data available.')\n",
    "        except Exception as e:\n",
    "            print(f'Error analyzing correlations: {e}')\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "analyze_button.on_click(analyze_correlations)\n",
    "display(widgets.VBox([pool_select, days, analyze_button, corr_output]))\n",
    "\n",
    "# Automatically try to update pool options if CSV exists\n",
    "update_pool_options(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Use the widgets above to select parameters and fetch pools.\n",
    "- Results are saved to a CSV file for further analysis.\n",
    "- Select multiple pools to compute price correlations over the specified number of days.\n",
    "- Ensure API keys are set in the `.env` file for CoinGecko and Moralis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
